\chapter{深度神经网络简介}

本章介绍实践中使用的最简单的深度学习架构，即多层感知机（MLP）。我们将讨论其各种组件、逼近不同正则性函数的能力（通用逼近结果），以及各种训练范式，以学习各种参数（和超参数）而不对训练数据集过拟合。

\section{MLP 架构}

设我们的目标是使用 MLP 逼近函数 $f: x \in \mathbb{R}^d \mapsto y \in \mathbb{R}^D$，我们将 MLP 记为 $F$。MLP 的计算单元称为人工神经元（artificial neurons），它们堆叠在多个连续层中。$F$ 的第零层称为源层（source layer），它不是计算层，仅负责向网络提供输入（维度为 $d$）。$F$ 的最后一层称为输出层（output layer），它输出网络的预测（维度为 $D$）。其间的每一层称为隐藏层（hidden layer）。一层中的神经元数量定义了该层的宽度（width）。具有 2 个隐藏层的 MLP 示意图如图 2.1 所示。

为了理解 MLP 内部的操作，让我们定义一些记号。我们考虑一个具有 $L$ 个隐藏层的网络（因此总共有 $L+2$ 层），第 $l$ 层的宽度记为 $H_l$，其中 $l = 0,1,\dots,L+1$。注意，为与我们要逼近的目标函数 $f$ 一致，我们必须有 $H_0 = d$ 和 $H_{L+1} = D$。设第 $l$ 层的输出向量为 $x^{(l)} \in \mathbb{R}^{H_l}$，它将作为下一层的输入。我们设 $x^{(0)} = x \in \mathbb{R}^d$，这是源层提供的输入信号。在每一层 $l$（$1 \leq l \leq L+1$）中，第 $i$ 个神经元对该层输入 $x^{(l-1)}$ 执行仿射变换（affine transformation），后跟非线性变换（non-linear transformation）：

\[
x^{(l)}_i = \sigma \left( \sum_{j=1}^{H_{l-1}} W^{(l)}_{ij} x^{(l-1)}_j + b^{(l)}_i \right), \quad 1 \leq i \leq H_l
\]

其中 $W^{(l)}_{ij}$ 和 $b^{(l)}_i$ 分别为第 $l$ 层第 $i$ 个神经元的权重（weights）和偏置（bias）。函数 $\sigma(\cdot)$ 称为激活函数（activation function），它在帮助网络表示非线性复杂函数方面起着关键作用。如果我们设 $W^{(l)} \in \mathbb{R}^{H_{l-1} \times H_l}$ 为第 $l$ 层的权重矩阵，$b^{(l)} \in \mathbb{R}^{H_l}$ 为第 $l$ 层的偏置向量，那么我们可以将整个层的操作重写为：

\[
x^{(l)} = \sigma \left( A^{(l)}(x^{(l-1)}) \right), \quad A^{(l)}(x^{(l-1)}) = W^{(l)} x^{(l-1)} + b^{(l)}
\]

其中激活函数是逐分量应用的。因此，整个网络 $F: \mathbb{R}^d \mapsto \mathbb{R}^D$ 可以数学上视为交替的仿射变换和逐分量激活的复合：

\[
F(x) = A^{(L+1)} \circ \sigma \circ A^{(L)} \circ \sigma \circ A^{(L-1)} \circ \cdots \circ \sigma \circ A^{(1)}(x).
\]

我们在这里做几点说明：

1. 为表示的简单起见，我们假设所有层使用相同的激活函数。然而，这不是严格规则。事实上，最近有证据表明，从一层到另一层交替激活函数会导致更好的神经网络 [119]。
2. 在 (2.3) 中所示的网络公式中，输出层是纯仿射的。有时，输出层末尾可能有一个输出函数 $O$，通常用于将输出重新格式化为合适的形式。我们将在本章后面看到此类函数的示例。
3. 我们使用网络深度（depth）一词来表示 MLP 中的计算层数，即隐藏层数加上输出层，按照上述记号为 $L+1$。

网络的可学习参数是所有权重和偏置，我们表示为：

\[
\theta = \{ W^{(l)}, b^{(l)} \}_{l=1}^{L+1} \in \mathbb{R}^{N_\theta}
\]

其中 $N_\theta$ 表示网络的总参数数量。网络 $F(x; \theta)$ 表示一个参数化函数族，其中 $\theta$ 需要适当选择，使得网络在输入 $x$ 处逼近目标函数 $f(x)$。

\textbf{问题 2.1.1} 证明 $N_\theta = \sum_{l=1}^{L+1} (H_{l-1} + 1) H_l$。

\begin{mycomment}
MLP 的架构本质上是仿射变换和非线性激活的交替复合。这使得网络能够表示复杂的非线性函数，而不仅仅是线性映射。如果没有激活函数，整个网络将退化为单一的仿射变换。
\end{mycomment}

\section{激活函数}

激活函数可能是 MLP 中最重要的组件。文献中提供了大量激活函数，每种都有其优点和缺点。让我们看看其中一些选项（另见图 2.2）。

\subsection{线性激活}

最简单的激活对应于 $\sigma(\xi) = \xi$。此函数的一些特征是：

• 该函数无限光滑，但二阶导数之外的所有导数均为零。
• 函数的范围是 $(-\infty, \infty)$。
• 在所有层中使用线性激活函数会将整个网络简化为输入 $x$ 的单一仿射变换。换言之，网络将只是目标函数 $f$ 的线性逼近，如果 $f$ 是非线性的，这将没有用处。

\subsection{修正线性单元（ReLU）}

此函数是分段线性的，定义为：

\[
\sigma(\xi) = \max\{0, \xi\} = 
\begin{cases} 
\xi, & \text{if } \xi \geq 0 \\ 
0, & \text{if } \xi < 0.
\end{cases}
\]

这是实践中最受欢迎的激活函数之一。此函数的一些特征是：

• 该函数是连续的，而其导数将是分段常数的，在 $\xi=0$ 处有跳跃。二阶导数将是集中在 $\xi=0$ 处的 Dirac 函数。换言之，高阶导数（大于 1）未定义。
• 函数的范围是 $[0, \infty)$。

\subsection{泄漏 ReLU（Leaky ReLU）}

ReLU 激活会导致如果神经元的仿射变换为负，则该神经元的输出为零。这可能会在训练神经网络时导致“死亡神经元”（dying neurons）现象 [62]，其中神经元完全从网络中退出，不再对最终预测贡献。为克服这一挑战，设计了泄漏版本的 ReLU：

\[
\sigma(\xi; \alpha) = 
\begin{cases} 
\xi, & \text{if } \xi \geq 0 \\ 
\alpha \xi, & \text{if } \xi < 0
\end{cases}
\]

其中 $\alpha$ 成为网络的超参数。此函数的一些特征是：

• Leaky ReLU 的导数行为与 ReLU 的相同。然而，一阶导数（除 $\xi=0$ 外）是非零的。
• 函数的范围是 $(-\infty, \infty)$。

\subsection{Logistic 函数}

Logistic 或 Sigmoid 激活函数由下式给出：

\[
\sigma(\xi) = \frac{1}{1 + e^{-\xi}}
\]

并具有以下性质：

• 该函数无限光滑且单调。
• 函数的范围是 $(0,1)$，即函数是有界的。此类函数对于表示概率很有用。它也用于表示有界的输出函数。
• 由于导数在远离 $\xi=0$ 时迅速衰减为零，此激活函数可能导致网络训练算法收敛缓慢。

\subsection{Tanh}

Tanh 函数可以视为 Logistic 函数的对称扩展：

\[
\sigma(\xi) = \frac{e^\xi - e^{-\xi}}{e^\xi + e^{-\xi}}
\]

并具有以下性质：

• 该函数无限光滑且单调。
• 函数的范围是 $(-1,1)$，即函数是有界的。注意，它将零输入映射为零，同时将正（负）输入推向 $+1$（$-1$）。
• 与 Logistic 函数类似，Tanh 的导数在远离 $\xi=0$ 时迅速衰减为零，因此可能导致训练缓慢。

\subsection{正弦（Sine）}

最近，正弦函数，即 $\sigma(\xi) = \sin(\xi)$ 被提出作为一种高效的激活函数 [100]。它具有上述所有激活函数的最佳特征：

• 该函数无限光滑。
• 函数的范围是 $[-1,1]$，即函数是有界的。
• 此函数的任何导数都不会衰减为零。
• 使用此激活函数的网络可以视为傅里叶级数的非线性泛化。
• 使用此激活函数已在函数的隐式表示中取得了令人印象深刻的结果 [100]。

\textbf{问题 2.2.1} 你能想出一个使用正弦激活函数的 MLP 架构，导致与傅里叶级数展开非常相似的逼近吗？

\textbf{备注 2.2.1} 还有许多其他激活函数在使用中，激活函数的全面列表可在 [25] 中找到。

\begin{mycomment}
激活函数的选择对网络的表达能力和训练稳定性至关重要。ReLU 及其变体因其简单性和有效性而广泛使用，但对于某些应用（如周期性函数），Sine 等新激活函数可能更优。
\end{mycomment}

\section{网络的表达性}

让我们尝试理解 MLP 中增加 $N_\theta$ 的效果。在流行说法中，这被称为检查网络表达性（expressivity）的增加效果。为了看到这一点，让我们使用 ReLU 激活函数考虑一个简单示例，即 $\sigma(\xi) = \max\{\xi, 0\}$。我们设 $d = D = 1$，$L=1$ 和参数：

\[
W^{(1)} = \begin{bmatrix} 2 \\ 1 \end{bmatrix}, \quad b^{(1)} = \begin{bmatrix} -2 \\ 0 \end{bmatrix}, \quad W^{(2)} = \begin{bmatrix} 1 & 1 \end{bmatrix}, \quad b^{(2)} = 0.
\]

如图 2.3a 所示。然后各个层的输出为：

\[
x^{(1)}_1 = \max\{2x^{(0)}_1 - 2, 0\}, \quad x^{(1)}_2 = \max\{x^{(0)}_1, 0\}, \quad x^{(2)}_1 = \max\{2x^{(0)}_1 - 2, 0\} + \max\{x^{(0)}_1, 0\}.
\]

注意，虽然隐藏层输出 $x^{(1)}$（见图 2.3b, c）只有一个拐角/弯折，最终输出却有两个弯折（见图 2.3d）。

我们将此公式推广到一个更大的网络，具有 $L$ 个隐藏层，每个宽度为 $H$。那么可以预期 $x^{(1)}_i$，$1 \leq i \leq H$ 将有一个单一弯折，弯折的位置和角度取决于隐藏层每个神经元的权重和偏置。向量 $x^{(1)}$ 被传递到下一个隐藏层，其中每个神经元将组合这些单一弯折，并给出可能具有 $H$ 个弯折的输出。同样，第二个隐藏层每个神经元输出中的 $H$ 个弯折的位置和角度将是不同的。弯折的位置不同是因为每个神经元允许不同的偏置，因此可以诱导不同的移位。继续这个论证，可以预期弯折的数量在通过各种宽度为 $H$ 的隐藏层时增加为 $H$、$H^2$、$H^3$。一般来说，总弯折数可以增长为 $H^L$。此外，通过适当选择网络中的权重和偏置，可以选择弯折的位置，以及弯折之间段的斜率。因此，可以通过生成分段连续逼近来逼近任何连续函数。换言之，随着网络深度（和宽度）的增加，网络的表达能力会增强。

作为说明，在图 2.4 中，我们展示了使用具有不同深度（以 $L$ 表示）和固定隐藏层宽度 $H=3$ 的神经网络逼近 $f(x) = \sin(2\pi x)$，$x \in [0,1]$，在 50 个等间距（在 $x$ 中）训练样本上训练。注意，随着深度增加，网络发展出更多弯折，允许它足够弯曲以更好地逼近目标函数。

\subsection{通用逼近结果}

为了以数学严谨的方式量化网络的表达性，我们来看一些关于 MLP 逼近性质的结果。对于这些结果，我们假设 $K \subset \mathbb{R}^d$ 是一个闭有界集。注意，这是一个合理要求，因为在大多数情况下，输入数据被缩放到某个闭有界集中。例如，$K = [0,1]^d$，即 $d$ 维立方体。

\textbf{定理 2.3.1} (Pinkus [83]) 设 $f: K \to \mathbb{R}$，即 $D=1$，是一个连续函数。那么给定 $\epsilon > 0$，存在一个具有单一隐藏层（$L=1$）、任意宽度 $H$ 和非多项式连续激活 $\sigma$ 的 MLP，使得：

\[
\max_{x \in K} |F(x; \theta) - f(x)| \leq \epsilon.
\]

此定理表明，如果允许选择任意大的宽度，一个具有单一隐藏层的网络可以以任何特定点误差逼近任何连续函数。

\textbf{定理 2.3.2} (Kidger [49]) 设 $f: K \to \mathbb{R}^D$ 是一个连续向量值函数。那么给定 $\epsilon > 0$，存在一个具有任意隐藏层数 $L$、每个宽度 $H \geq d + D + 2$、连续激活 $\sigma$（具有一些额外的温和条件）的 MLP，使得：

\[
\max_{x \in K} \| F(x; \theta) - f(x) \| \leq \epsilon.
\]

这里 $\|\cdot\|$ 表示向量的欧几里得 2-范数。

此定理为具有固定有限宽度和任意大深度的网络提供了类似结果。

\textbf{定理 2.3.3} (Yarotsky [119]) 设 $f: K \to \mathbb{R}$ 是一个具有两个连续导数的函数，即 $f \in C^2(K)$。考虑具有 ReLU 激活和 $H \geq 2d + 10$ 的 MLP。那么存在此配置的网络，使得误差收敛为：

\[
\max_{x \in K} |F(x; \theta) - f(x)| \leq C (N_\theta)^{-4}
\]

其中 $C$ 是依赖于网络参数数量的常数。

上述定理指定了网络逼近误差如何随着网络参数数量 $N_\theta$ 的变化而变化。

像上述那样的理论结果有助于揭开神经网络的“黑箱”性质，并作为设计网络架构的有用实际指南。

\begin{mycomment}
通用逼近定理解释了为什么 MLP 可以逼近任意复杂函数，但实际性能取决于训练和正则化。深度与宽度之间的权衡在实践中很重要。
\end{mycomment}

\section{神经网络的训练、验证和测试}

现在我们对 MLP 的架构有了更好的理解，我们想讨论如何设置这些网络的参数来逼近某个目标函数。我们将讨论限制在监督学习框架中。

假设我们给定一个成对样本数据集 $S = \{(x_i, y_i): 1 \leq i \leq N\}$，对应于目标函数 $f: x \mapsto y$。我们希望使用神经网络逼近此函数：

\[
F(x; \theta, \Theta)
\]

其中 $\theta$ 是之前定义的网络参数，而 $\Theta$ 对应于网络的超参数（hyper-parameters）。这些包括深度 $L+1$、宽度 $H$、激活函数类型 $\sigma$ 等。设计鲁棒网络的策略涉及三个步骤：

1. 在训练阶段找到 $\theta$ 的最优值（对于固定的 $\Theta$）。
2. 在验证阶段找到 $\Theta$ 的最优值。
3. 在测试阶段测试网络在未见数据上的性能。

为了完成这些任务，通常将数据集 $S$ 分成三个不同的部分：具有 $N_{\text{train}}$ 个样本的训练集、具有 $N_{\text{val}}$ 个样本的验证集和具有 $N_{\text{test}}$ 个样本的测试集，其中 $N = N_{\text{train}} + N_{\text{val}} + N_{\text{test}}$。通常，使用大约 60\% 的样本作为训练样本，20\% 作为验证样本，其余 20\% 用于测试。

分割数据集是必要的，因为神经网络是高度过参数化的函数。可用于建模数据的自由度很大，可能导致数据过拟合。这发生在数据中的误差或噪声比底层输入-输出关系更驱动网络行为时。因此，一部分数据用于确定 $\theta$，另一部分用于确定超参数 $\Theta$。其余数据保留用于测试训练网络在未使用数据上的性能，即网络的泛化能力。

现在让我们进一步详细讨论这个分割在三个阶段中的使用：
训练：训练网络使用训练集 $S_{\text{train}}$ 来解决以下优化问题：找到

\[
\theta^* = \arg\min_\theta \Pi_{\text{train}}(\theta), \quad \Pi_{\text{train}}(\theta) = \frac{1}{N_{\text{train}}} \sum_{i=1}^{N_{\text{train}}} \| y_i - F(x_i; \theta, \Theta) \|^2
\]

对于某些固定的 $\Theta$。最优 $\theta^*$ 使用合适的基于梯度的算法获得（稍后讨论）。函数 $\Pi_{\text{train}}$ 称为训练损失函数。在上面的示例中，我们使用了均方损失函数。后来我们将考虑其他类型的损失函数。

验证：网络的验证涉及使用验证集 $S_{\text{val}}$ 来解决以下优化问题：找到

\[
\Theta^* = \arg\min_\Theta \Pi_{\text{val}}(\Theta), \quad \Pi_{\text{val}}(\Theta) = \frac{1}{N_{\text{val}}} \sum_{i=1}^{N_{\text{val}}} \| y_i - F(x_i; \theta^*, \Theta) \|^2.
\]

最优 $\Theta^*$ 使用诸如（随机或张量）网格搜索的技术获得。注意，最优 $\theta^*$ 取决于 $\Theta$ 的选择，即 $\theta^* = \theta^*(\Theta)$。为记号方便，我们在这里抑制了此依赖。

测试：一旦获得“最佳”网络，由 $\theta^*$ 和 $\Theta^*$ 表征，它在测试集 $S_{\text{test}}$ 上评估，以估计网络在第一两个阶段未使用数据上的性能。

\[
\Pi_{\text{test}} = \frac{1}{N_{\text{test}}} \sum_{i=1}^{N_{\text{test}}} \| y_i - F(x_i; \theta^*, \Theta^*) \|^2.
\]

此测试误差也称为网络的（近似）泛化误差。

让我们看一个例子来更好地理解如何获得这样的网络。

\textbf{示例 2.4.1} 让我们考虑一个 MLP，其中所有超参数都是固定的，除了以下灵活选择：

\[
\sigma \in \{\text{ReLU}, \tanh\}, \quad L \in \{10, 20\}.
\]

我们使用以下算法：

1. 对于每个可能的 $\sigma, L$ 对：

(a) 找到 $\theta^* = \arg\min_\theta \Pi_{\text{train}}(\theta)$
(b) 使用此 $\theta^*$，评估 $\Pi_{\text{val}}(\Theta)$

2. 选择 $\Theta^*$ 为给出 $\Pi_{\text{val}}(\Theta)$ 最小值的那个。
3. 最后，为此 $\Theta^*$ 和对应的 $\theta^*$ 报告 $\Pi_{\text{test}}$。

\textbf{备注 2.4.1} 测试数据集的概念很重要，因为它在实践中经常被误用。在“正确”方法中，测试数据绝不应用于改进网络的性能。一旦网络（以优化的 $\theta$ 和 $\Theta$）在测试集上评估，进一步改变 $\theta$ 和 $\Theta$ 以改进测试性能可以视为“数据窥探”，测试集成为一个美化的验证集。

\begin{mycomment}
训练-验证-测试分割是防止过拟合的关键。实际中，交叉验证可以进一步优化超参数选择，但对于大型数据集，简单分割通常足够。
\end{mycomment}

\section{过拟合及其避免方法}

神经网络，尤其是 MLP，几乎总是过参数化的，即 $N_\theta \gg N_{\text{train}}$，其中 $N_{\text{train}}$ 是训练样本数量。这会导致高度非线性的网络模型，如图 2.5a 所示。在该图中，我们展示了标量值函数 $f$（黑曲线）接受标量输入 $x$ 的逼近。品红曲线表示网络预测，红点是噪声训练点，而蓝叉是噪声验证点。过参数化的 MLP 有拟合训练集中噪声的倾向，从而在验证（或任何未见）数据上性能差。我们希望避免这种过拟合。

首先，我们希望能够找出我们是否处于这种情况下。我们如何知道这一点？从图 2.5a 中可以得到线索，通过注意品红曲线（MLP 预测）和验证数据点（蓝叉）之间的距离。此距离远大于品红曲线和训练数据点之间的距离。这将转化为 $\Pi_{\text{val}} \gg \Pi_{\text{train}}$。因此，每当观察到验证和训练损失之间有较大差距时，都应该考虑是否在过拟合数据。

现在我们有识别过拟合的方法，下一个问题是怎样避免它？有几种方法；然而最流行的方法称为正则化（regularization），将在下面描述。

\subsection{正则化}

从图 2.5a 中，我们观察到黑曲线（我们想要的）和品红曲线（我们预测的）之间的一个显著差异是前者比后者平滑得多。即黑曲线相对于其自变量的导数远小于红曲线的导数。这告诉我们，我们希望训练 MLP 使得 $| \partial F(x) / \partial x |$ 小。正则化是实现这一目标的方法。

考虑网络的第一个隐藏层的输出，其输入是一个标量 $x$，

\[
x^{(1)}_1 = \sigma( W^{(1)}_{11} x + b^{(1)}_1 ),
\]

这给出

\[
\frac{\partial x^{(1)}_1}{\partial x} = \sigma'( W^{(1)}_{11} x + b^{(1)}_1 ) W^{(1)}_{11} \propto W^{(1)}_{11}.
\]

由于此导数与 $W^{(1)}_{11}$ 成比例，这告诉我们，如果我们限制 $W^{(1)}_{11}$ 的值，我们也会限制此导数的值。当然，这只是网络第一个隐藏层输出的导数。我们稍后将显示 MLP 最终输出的导数是此类导数的乘积。因此，如果我们可以控制此导数和其他类似导数，我们可以控制整体导数。最明显的方法是通过惩罚网络中权重的大值。这正是通过向损失函数添加正则化项来实现的。

最简单的正则化方法涉及向损失函数添加惩罚项：

\[
\Pi(\theta) \to \Pi(\theta) + \alpha \| \theta \|, \quad \alpha \geq 0
\]

其中 $\alpha$ 是正则化超参数，$\| \theta \|$ 是网络参数 $\theta$ 的合适范数。当向量 $\theta$ 的单个组件大时，此项也大。现在，除了找到最匹配训练数据的参数值外，我们还在寻找那些小的参数，从而导致 MLP 的输出更平滑。

让我们考虑一些常见的正则化类型：

• $l_2$ 正则化：这里我们使用 $l_2$ 范数在正则化项中

\[
\| \theta \| = \| \theta \|_2 = \left( \sum_{i=1}^{N_\theta} \theta_i^2 \right)^{1/2}.
\]

• $l_1$ 正则化：这里我们使用 $l_1$ 范数在正则化项中

\[
\| \theta \| = \| \theta \|_1 = \sum_{i=1}^{N_\theta} | \theta_i |,
\]

这促进 $\theta$ 的稀疏性。

在图 2.5b 中，我们描绘了正则化网络的预测。使用惩罚项，我们获得复杂度较低的网络，其预测更平滑。此外，我们注意到预测和验证点之间的不匹配降低了，但以略高的训练数据预测误差为代价。因此，我们现在有 $\Pi_{\text{val}} \approx \Pi_{\text{train}}$。在统计学习的术语中，此概念也称为偏差-方差权衡（bias-variance tradeoff）[45]，它表示随着预测模型的模型复杂度降低，偏差（通过用简单 $F$ 逼近复杂 $f$ 引入的误差）增加，而方差（$F$ 对训练数据变化的敏感性）减少。在实践中，这最好通过 $\Pi_{\text{val}}$ 和 $\Pi_{\text{train}}$ 来监控。如图 2.6 所示，对于复杂 $F$，$\Pi_{\text{val}}$ 通常高，而 $\Pi_{\text{train}}$ 低。随着模型复杂度降低（例如通过增加正则化参数 $\alpha$），$\Pi_{\text{val}}$ 降低而 $\Pi_{\text{train}}$ 增加。然而，通常有一个甜点，超出该点后，随着模型的进一步简化，$\Pi_{\text{val}}$ 和 $\Pi_{\text{train}}$ 都增加。这种情况称为欠拟合（underfitting），如果 $\alpha$ 选择太大可能会发生。因此，仔细选择 $\alpha$（图中标记为 $\alpha^*$）变得重要，以确保网络在训练集上表现良好，同时对未见数据泛化良好。

\begin{mycomment}
正则化是控制模型复杂度的核心工具。$l_2$ 正则化（也称权重衰减）在实践中很常见，因为它鼓励参数平滑分布，而 $l_1$ 促进稀疏模型。
\end{mycomment}

\section{梯度下降}

回想一下，我们希望在训练阶段解决最小化问题 $\theta^* = \arg\min \Pi(\theta)$。此最小化问题可以使用称为梯度下降（GD，也称为最陡下降）的迭代优化算法来解决。假设损失函数相对于 $\theta$ 足够光滑，考虑关于 $\theta_0$ 的截断 Taylor 展开：

\[
\Pi(\theta_0 + \Delta \theta) = \Pi(\theta_0) + \frac{\partial \Pi}{\partial \theta}(\theta_0) \cdot \Delta \theta + \frac{\partial^2 \Pi}{\partial \theta_i \partial \theta_j}(\hat{\theta}) \Delta \theta_i \Delta \theta_j
\]

对于 $\hat{\theta}$ 在 $\theta_0$ 的小邻域中。当 $\| \Delta \theta \|$ 小且假设 $\partial^2 \Pi / \partial \theta_i \partial \theta_j$ 有界时，我们可以忽略二阶项，只考虑逼近：

\[
\Pi(\theta_0 + \Delta \theta) \approx \Pi(\theta_0) + \frac{\partial \Pi}{\partial \theta}(\theta_0) \cdot \Delta \theta.
\]

为了尽可能降低损失函数的值与在 $\theta_0$ 处的值相比，即最小化 $\Delta \Pi = \Pi(\theta_0 + \Delta \theta) - \Pi(\theta_0)$，我们需要在梯度的相反方向选择步长 $\Delta \theta$，即：

\[
\Delta \theta = - \eta \frac{\partial \Pi}{\partial \theta}(\theta_0)
\]

其中步长 $\eta \geq 0$，也称为学习率（learning rate）。这是我们在验证阶段需要调优的另一个超参数。注意，通过将 (2.9) 代入 (2.8)，我们有：

\[
\Pi(\theta_0 + \Delta \theta) \approx \Pi(\theta_0) - \eta \left\| \frac{\partial \Pi}{\partial \theta}(\theta_0) \right\|_2^2 \leq \Pi(\theta_0).
\]

这是 GD 算法的核心，可以总结如下：

1. 初始化 $k=0$ 和 $\theta_0$
2. 而 $| \Pi(\theta_k) | > \epsilon_1$，做

(a) 评估 $\partial \Pi / \partial \theta (\theta_k)$
(b) 更新 $\theta_{k+1} = \theta_k - \eta \partial \Pi / \partial \theta (\theta_k)$
(c) 递增 $k = k+1$。

\subsection{收敛}

假设 $\Pi(\theta)$ 是凸的且可微的，其梯度是 Lipschitz 连续的，Lipschitz 常数为 $K$。那么对于 $\eta \leq 1/K$，GD 更新以速率收敛：

\[
\| \theta^* - \theta_k \|_2 \leq \frac{C}{k}.
\]

然而，在大多数场景中 $\Pi(\theta)$ 不是凸的。如果有多个极小值，那么 GD 喜欢选择哪种极小值？为了回答这个，考虑图 2.7 中所示的对于标量 $\theta$ 的损失函数，它有两个谷。假设每个谷中的 $\Pi(\theta)$ 剖面可以局部逼近为抛物线：

\[
\Pi(\theta) \approx \frac{1}{2} a (\theta - \theta^*)^2
\]

其中 $a > 0$ 是曲率，而 $\theta^*$ 是局部极小值。显然每个谷有不同的 $a$ 和 $\theta^*$ 值。注意左谷的曲率远小于右谷的曲率。让我们选择一个恒定学习率 $\eta$ 和任一谷中的起始值 $\theta_0$。那么，

\[
\frac{\partial \Pi}{\partial \theta}(\theta_0) = a (\theta_0 - \theta^*)
\]

GD 更新后的新点将是

\[
\theta_1 = \theta_0 - \eta a (\theta_0 - \theta^*) \iff (\theta_1 - \theta^*) = (\theta_0 - \theta^*)(1 - \eta a)
\]

类似地，很容易看到所有后续迭代可以写为

\[
(\theta_{k+1} - \theta^*) = (\theta_k - \theta^*)(1 - \eta a) = (\theta_0 - \theta^*)(1 - \eta a)^k
\]

如果 $|1 - \eta a| < 1$，迭代 $\theta_k$ 将收敛到 $\theta^*$。由于谷中 $a > 0$，我们将需要学习率满足以下条件

\[
-1 < 1 - \eta a \implies \eta a < 2.
\]

如果我们固定 $\eta$，那么对于收敛，我们需要局部曲率满足 $a < 2/\eta$。换言之，GD 将偏好收敛到具有平坦/小曲率的极小值，即它将偏好左谷的极小值。如果起始点在右谷，有机会我们将不断越过右极小值并从对面墙反弹，直到 GD 算法将 $\theta_k$ 投射出谷（图 2.7 中描绘）。之后它将进入具有较小曲率的左谷，并逐渐向其极小值移动。

虽然清楚 GD 偏好平坦极小值，但不清楚为什么平坦极小值更好。有经验证据表明，在平坦极小值处获得的参数值往往泛化更好，因此是首选 [47]。

\begin{mycomment}
梯度下降是优化基础，但实际中需注意学习率调度。平坦极小值的偏好解释了为什么噪声（如随机梯度下降）有时有助于泛化。
\end{mycomment}

\section{一些高级优化算法}

我们讨论了如何使用 GD 来解决训练神经网络涉及的优化问题。让我们看看一些由 GD 启发的先进和流行优化技术。

一般来说，大多数优化算法的更新公式使用以下公式

\[
[\theta_{k+1}]_i = [\theta_k]_i - [\eta_k]_i [g_k]_i, \quad 1 \leq i \leq N_\theta.
\]

其中我们使用了记号，对于任何向量 $a$，$[a]_i$ 表示第 $i$ 个组件。在上面的等式中 $[\eta_k]_i$ 是逐分量学习率，向量值函数 $g$ 依赖于/逼近梯度。注意，学习率允许依赖于迭代号 $k$。

GD 方法可以通过识别以下来表示使用 (2.10)

\[
[\eta_k]_i = \eta, \quad g_k = \frac{\partial \Pi}{\partial \theta}(\theta_k).
\]

GD 方法的一个问题是，如果 $\eta$ 未适当选择，收敛到极小值可能相当慢。例如，考虑图 2.8 中所示的目标函数景观，沿 $[\theta]_2$ 方向的梯度比沿 $[\theta]_1$ 方向的更陡峭。如果我们从图中红十字标记的点开始，那么如果 $\eta$ 太大（但仍在稳定界限内），更新将不断 zig-zagging 向极小值前进。理想情况下，对于图 2.8 中所示的特定情况，我们希望步骤沿 $[\theta]_1$ 方向比沿 $[\theta]_2$ 方向采取更长的跨度，从而更快到达极小值。

让我们看看两个能够克服 GD 面临的一些问题的流行方法。

\subsection{动量方法}

动量方法使用梯度的历史，而不是仅前一步的梯度。更新公式由 (2.10) 给出，其中

\[
[\eta_k]_i = \eta, \quad g_k = \beta_1 g_{k-1} + (1 - \beta_1) \frac{\partial \Pi}{\partial \theta}(\theta_k), \quad g_{-1} = 0
\]

其中 $g_k$ 是梯度的加权移动平均。此加权预期平滑图 2.8 中看到的 zig-zagging，通过取消沿 $[\theta]_2$ 方向的梯度分量，并导致更新更平滑地向极小值移动。$\beta_1$ 的常用值为 0.9。

\subsection{Adam}

Adam 优化算法（“adaptive moment estimation”的缩写）由 Kingma 和 Ba [50] 引入，使用梯度的历史以及梯度的二阶矩（这是幅度的度量）。对于初始学习率 $\eta$，更新再次由 (2.10) 给出，其中

\[
g_k = \beta_1 g_{k-1} + (1 - \beta_1) \frac{\partial \Pi}{\partial \theta}(\theta_k)
\]

\[
[G_k]_i = \beta_2 [G_{k-1}]_i + (1 - \beta_2) \left( \frac{\partial \Pi}{\partial \theta_i}(\theta_k) \right)^2
\]

\[
[\eta_k]_i = \frac{\eta}{\sqrt{[G_k]_i} + \epsilon}
\]

在上面的等式中，$g_k$ 和 $G_k$ 分别是梯度和梯度平方的加权运行平均。超参数的推荐值为 $\beta_1 = 0.9$，$\beta_2 = 0.999$ 和 $\epsilon = 10^{-8}$。注意，每个组件的学习率不同。特别是，梯度幅越大，其学习率越小。回溯到图 2.8 中的示例，这意味着相对于 $\theta_1$，$\theta_2$ 的学习率更小，因此将有助于缓解优化算法的 zig-zag 路径。

\textbf{备注 2.7.1} Adam 算法还对 $g_k$ 和 $G_k$ 有额外的校正步骤，以提高算法效率。详见 [50]。

\subsection{随机优化}

我们注意到训练损失可以重写为

\[
\Pi(\theta) = \frac{1}{N_{\text{train}}} \sum_{i=1}^{N_{\text{train}}} \Pi_i(\theta), \quad \Pi_i(\theta) = \| y_i - F(x_i; \theta, \Theta) \|^2
\]

因此，损失函数的梯度是

\[
\frac{\partial \Pi}{\partial \theta}(\theta) = \frac{1}{N_{\text{train}}} \sum_{i=1}^{N_{\text{train}}} \frac{\partial \Pi_i}{\partial \theta}(\theta)
\]

然而，求梯度的求和可能非常昂贵，因为 $N_{\text{train}}$ 通常很大，$N_{\text{train}} \sim 10^6$。规避此问题的一个简单方法是使用以下更新公式（这里显示为 GD 方法）

\[
\theta_{k+1} = \theta_k - \eta_k \frac{\partial \Pi_i}{\partial \theta}(\theta_k),
\]

其中 $i$ 对于每个更新步 $k$ 随机选择。这称为随机梯度下降（stochastic gradient descent）。值得注意的是，此修改算法确实收敛，假设 $\Pi_i(\theta)$ 是凸的且可微的，且 $\eta_k \sim 1/\sqrt{k}$ [72]。

为了说明为什么 $\eta_k$ 需要衰减，考虑对于 $\theta \in \mathbb{R}^2$ 的玩具函数

\[
\Pi_1(\theta) = ([\theta]_1 - 1)^2 + ([\theta]_2 - 1)^2, \quad \Pi_2(\theta) = ([\theta]_1 + 1)^2 + 0.5 ([\theta]_2 - 1)^2,
\]

\[
\Pi_3(\theta) = 0.7 ([\theta]_1 + 1)^2 + 0.5 ([\theta]_2 + 1)^2, \quad \Pi_4(\theta) = 0.7 ([\theta]_1 - 1)^2 + \frac{1}{2} ([\theta]_2 + 1)^2,
\]

\[
\Pi(\theta) = \frac{1}{4} (\Pi_1(\theta) + \Pi_2(\theta) + \Pi_3(\theta) + \Pi_4(\theta)).
\]

这些函数的等高线图如图 2.9a 所示，其中黑等高线对应于 $\Pi(\theta)$。注意 $\theta^* = (0,0)$ 是 $\Pi(\theta)$ 的唯一极小值。我们考虑使用 SGD 算法，恒定学习率 $\eta_k = 0.4$ 和衰减学习率 $\eta_k = 0.4 / \sqrt{k}$。从 $\theta_0 = (-1.0, 2.0)$ 开始，并为每个步 $k$ 随机选择 $i \in \{1,2,3,4\}$，我们运行算法 10,000 次迭代。每个学习率的第一个 10 步如图 2.9a 所示。我们可以清楚地看到，没有学习率衰减，SGD 算法不断越过极小值。事实上，这种行为继续所有未来迭代，如图 2.9b 所示，其中更新的范数不衰减（我们期望它衰减到 $|\theta^*| = 0$）。另一方面，如果学习率衰减为 $1/\sqrt{k}$，我们快速移动到更接近 $\theta^*$。

在接近极小值时减少步长原因是远离 $\Pi$ 的极小值，$\Pi$ 和所有单个 $\Pi_i$ 的梯度向量对齐相当好。然而，当我们接近 $\Pi$ 的极小值时，情况并非如此，因此需要采取更小的步长，以免被抛到远离极小值的区域。

在实践中，不使用随机优化算法的原因如下：

1. 虽然损失函数随着迭代次数衰减，但它在极小值附近以混乱方式波动，从未达到极小值。
2. 虽然一次处理所有样本可能计算昂贵，但一次处理单个样本严重未利用计算和内存资源。

然而，可以通过使用小批量优化（mini-batch optimization）来妥协。在此策略中，$N_{\text{train}}$ 个样本的数据集被分割成 $N_{\text{batch}}$ 个不相交子集，称为小批量。每个小批量包含 $N_{\text{train}} = N_{\text{train}} / N_{\text{batch}}$ 个样本，这也称为批量大小（batch size）。因此，损失函数的梯度可以逼近为

\[
\frac{\partial \Pi}{\partial \theta}(\theta) = \frac{1}{N_{\text{train}}} \sum_{i=1}^{N_{\text{train}}} \frac{\partial \Pi_i}{\partial \theta}(\theta) \approx \frac{1}{N_{\text{train}}} \sum_{i \in \text{batch}(j)} \frac{\partial \Pi_i}{\partial \theta}(\theta).
\]

注意，取 $N_{\text{batch}} = 1$ 导致原始优化算法，而取 $N_{\text{batch}} = N_{\text{train}}$ 给出随机梯度下降算法。通常选择批量大小以最大化一次可以加载到 RAM 中的数据量。我们定义一个 epoch 为通过完整训练集的所有样本（或小批量）的完整一遍。以下描述小批量随机优化算法：

1. 对于 epoch = 1, \dots, J

(a) 随机洗牌完整训练集
(b) 创建 $N_{\text{batch}}$ 个小批量

(c) 对于 $i = 1, \dots, N_{\text{batch}}$

i. 使用 (2.14) 评估批量梯度。
ii. 使用此梯度和您喜欢的优化算法（梯度下降、动量或 Adam）更新 $\theta$。

\textbf{备注 2.7.2} 有一项有趣的研究 [113] 表明随机梯度下降可能实际上有助于选择泛化更好的极小值。在该研究中，作者证明 SGD 偏好曲率更均匀的极小值。即，损失函数每个组件曲率的分布是尖锐的并集中在小值周围。这与整体曲率可能小但损失函数每个组件曲率的分布更分散的极小值相反。然后他们继续显示（经验上）更均匀的极小值比其异质对应物泛化更好。

\begin{mycomment}
Adam 是当前深度学习中最流行的优化器，因为它结合了动量和自适应学习率。小批量是实际训练的关键，能平衡计算效率和噪声益处。
\end{mycomment}

\section{使用反向传播计算梯度}

训练算法的最后一部分是我们需要理解在训练网络时如何实际评估梯度。回想第 $l+1$ 层的输出 $x^{(l+1)}$ 由下式给出

仿射变换： $\xi^{(l+1)}_i = \sum_{j=1}^{H_l} W^{(l+1)}_{ij} x^{(l)}_j + b^{(l+1)}_i$，$1 \leq i \leq H_{l+1}$

非线性变换： $x^{(l+1)}_i = \sigma( \xi^{(l+1)}_i )$，$1 \leq i \leq H_{l+1}$。

给定训练样本 $(x, y)$，设 $x^{(0)} = x$。损失/目标函数的值（对于此特定样本）可以使用前向传播评估：

1. 对于 $l = 1, \dots, L+1$

(a) 使用 (2.15) 评估 $\xi^{(l)}$。
(b) 使用 (2.16) 评估 $x^{(l)}$。

2. 评估给定样本的损失函数

\[
\Pi(\theta) = \| y - F(x; \theta, \Theta) \|^2.
\]

此操作可以简洁地写成计算图形式，如图 2.10 所示。在该图中，图的下部表示损失函数 $\Pi$ 的评估。

我们当然需要为训练集中的所有样本（或用于随机优化的小批量）重复此步骤。为简单起见，我们将讨论限制在单个样本的损失及其梯度的评估。

为了更新网络参数，我们需要 $\partial \Pi / \partial \theta$，或更精确地说，$\partial \Pi / \partial W^{(l)}$，$\partial \Pi / \partial b^{(l)}$ 对于 $1 \leq l \leq L+1$。我们将通过首先推导 $\partial \Pi / \partial \xi^{(l)}$ 和 $\partial \Pi / \partial x^{(l)}$ 的表达式来推导这些导数的表达式。

从计算图中很容易看到网络中的每个隐藏变量如何转换为下一个。认识到这一点，并反复应用链式法则得到

\[
\frac{\partial \Pi}{\partial \xi^{(l)}} = \frac{\partial \Pi}{\partial x^{(L+1)}} \cdot \frac{\partial x^{(L+1)}}{\partial \xi^{(L+1)}} \cdot \frac{\partial \xi^{(L+1)}}{\partial x^{(L)}} \cdot \cdots \frac{\partial x^{(l+1)}}{\partial \xi^{(l+1)}} \cdot \frac{\partial \xi^{(l+1)}}{\partial x^{(l)}} \cdot \frac{\partial x^{(l)}}{\partial \xi^{(l)}}.
\]

为了评估此表达式，我们需要评估以下项：

\[
\frac{\partial \Pi}{\partial x^{(L+1)}} = -2 (y - x^{(L+1)})
\]

\[
\frac{\partial \xi^{(l+1)}}{\partial x^{(l)}} = W^{(l+1)}
\]

\[
\frac{\partial x^{(l)}}{\partial \xi^{(l)}} = S^{(l)} \equiv \diag[ \sigma'(\xi^{(l)}_1), \dots, \sigma'(\xi^{(l)}_{H_l}) ],
\]

其中最后两个关系对任何网络层 $l$ 成立，$H_l$ 是该特定层的宽度，$\sigma'$ 表示激活相对于其自变量的导数。将这些关系代入 (2.17)，我们得到

\[
\frac{\partial \Pi}{\partial \xi^{(l)}} = \frac{\partial \Pi}{\partial x^{(L+1)}} \cdot S^{(L+1)} \cdot W^{(L+1)} \cdot \cdots S^{(l+1)} \cdot W^{(l+1)} \cdot S^{(l)}.
\]

现在考虑向量 $v = a \cdot M$，其中 $a$ 是另一个向量，$M$ 是矩阵。那么我们可以写 $v = a^T M = M^T a$。在上面的等式中使用这个，认识到一系列矩阵乘积的转置等于矩阵转置的乘积（逆序），并认识到 $\sum^{(l)}$ 是对角的因此是对称的，我们最终得到

\[
\frac{\partial \Pi}{\partial \xi^{(l)}} 
= S^{(l)} W^{(l+1)^{T}} S^{(l+1)} \cdots W^{(L+1)^{T}} S^{(L+1)} \big[-2 (y - x^{(L+1)})\big].
\]


此评估也可以表示为计算图。事实上，如图 2.10 所示，它可以附加到原始图，其中此部分计算出现在图的上行。注意，我们现在在反向遍历。因此名为反向传播（back propagation）。

最后一步是评估 $\partial \Pi / \partial W^{(l)}$ 的显式表达式。这可以通过认识到来完成，

\[
\frac{\partial \Pi}{\partial W^{(l)}} = \frac{\partial \Pi}{\partial \xi^{(l)}} \cdot \frac{\partial \xi^{(l)}}{\partial W^{(l)}} = \frac{\partial \Pi}{\partial \xi^{(l)}} \otimes x^{(l-1)},
\]

其中 $[x \otimes y]_{ij} = x_i y_j$ 是外积。因此，为了评估 $\partial \Pi / \partial W^{(l)}$，我们需要在前向阶段评估的 $x^{(l-1)}$ 和在反向传播中评估的 $\partial \Pi / \partial \xi^{(l)}$。

\textbf{备注 2.8.1} 估计算 MLP 可学习参数的损失函数及其导数的计算成本是有指导意义的。让我们为具有 $L$ 个隐藏层、每个宽度 $H$ 的 MLP 计算此估计。计算损失函数的成本与图 2.10 中图的下部分支相关。在该图中，每个仿射变换的成本是 $O(H^2)$ flops，而每个激活层的成本是 $O(H)$。由于我们有 $L$ 这样的层，计算损失函数的主导成本缩放为 $O(H^2 L)$。计算损失函数导数的成本通过估计算图上分支的成本加上计算外积的成本来估计。前者由每个层 $W^{(l)\,T}$ 的矩阵-向量乘积主导，并缩放为 $O(H^2)$。计算外积的成本与每个矩阵的条目数乘以矩阵数缩放，因此也由 $O(H^2 L)$ 给出。因此，计算损失函数导数的成本缩放为 $O(2 H^2 L)$，这是与计算损失函数本身相同阶的。这一事实至关重要，使得使用合理计算资源训练 MLP 成为可能。

\textbf{问题 2.8.1} 你能推导一组类似的表达式和相应的算法来评估 $\partial \Pi / \partial b^{(l)}$ 吗？

\textbf{问题 2.8.2} 你能推导网络输出相对于其输入的导数 $\partial x^{(L+1)} / \partial x^{(0)}$ 的显式表达式吗？这是一个非常有用的量，在算法如物理信息神经网络（见第 5 章）和 Wasserstein 生成对抗网络（见第 7 章）中找到应用。

\begin{mycomment}
反向传播是深度学习的核心算法，其效率使得训练深层网络可行。理解计算图有助于调试和扩展到其他架构。
\end{mycomment}

\section{回归与分类}

到目前为止，给定标签数据集 $S = \{(x_i, y_i): 1 \leq i \leq N\}$，我们考虑了两种类型的损失

• 均方误差 (MSE)

\[
\Pi(\theta) = \frac{1}{N_{\text{train}}} \sum_{i=1}^{N_{\text{train}}} \| y_i - F(x_i; \theta, \Theta) \|^2.
\]

• 均绝对误差 (MAE)

\[
\Pi(\theta) = \frac{1}{N_{\text{train}}} \sum_{i=1}^{N_{\text{train}}} \| y_i - F(x_i; \theta, \Theta) \|.
\]

具有上述损失的神经网络可用于解决各种回归问题，其中底层函数高度非线性，输入/输出是多维的。

\textbf{示例 2.9.1} 给定房屋/公寓特征，如邮政编码、卧室/浴室数量、地毯面积、建筑年龄等，预测结果，如市场售价，或上市天数。

现在让我们考虑一些分类问题，其中网络的输出通常位于离散有限集中。

\textbf{示例 2.9.2} 给定 COVID-19 患者的症状和血液标志物，预测患者是否需要住院治疗、重症监护或死亡。

[文档在此处截断，总页数 31 页]

\begin{mycomment}
回归使用连续输出和 MSE/MAE 损失，而分类使用离散输出和交叉熵损失。softmax 函数常用于多类分类，以产生概率分布。
\end{mycomment}

\newpage